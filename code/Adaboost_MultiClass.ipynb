{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f728388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html\n",
    "#https://github.com/jinxin0924/multi-adaboost/blob/master/multi_AdaBoost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93940bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765b67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_quantiles(\n",
    "    n_samples=13000, n_features=10, n_classes=2, random_state=1\n",
    ")\n",
    "\n",
    "y = np.where(y==0,-1,1)\n",
    "\n",
    "n_split = 3000\n",
    "\n",
    "X_train, X_test = X[:n_split], X[n_split:]\n",
    "y_train, y_test = y[:n_split], y[n_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c042eb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a binary \n",
    "y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aa747cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------- BINARY CLASSIFICATIONS ---------------------- #\n",
    "\n",
    "class BinaryClassAdaboost():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators:int):\n",
    "        \"\"\"\n",
    "        Initialialisation of Adaboost class\n",
    "        Parameters: \n",
    "            n_estimators: int:  number of weak learners \n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        self.estimator_errors = []\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model \n",
    "        Parameters: \n",
    "            X: array: data\n",
    "            y: array: vector of class labels where yi E Y= {1,..., k} and k = 2\n",
    "        \"\"\"\n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "              \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "\n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X, y, sample_weight=w_t)\n",
    "            #Get the predicted classes\n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            eps = self.error_wl(w_t, y_pred, y)\n",
    "        \n",
    "            # if the error of the weak learner is higher then 0.5 (worse then random guess) \n",
    "            #don't take into account this learner weight\n",
    "            if eps > 0.5:\n",
    "                break\n",
    "            \n",
    "            #Step 4: Calculate the performance of the weak learner\n",
    "            #Performance of the weak learner(α) = 0.5* ln (1 – error/error)\n",
    "            #Calculate alpha for this weak learner\n",
    "            alpha_t = 0.5 * np.log((1- eps) / eps)\n",
    "            \n",
    "            \n",
    "            #Step 5: Update weight\n",
    "            #With the alpha performance (α) the weights of the wrongly classified records are increased\n",
    "            #and the weights of the correctly classified records decreased.\n",
    "            y_temp = np.multiply(y, y_pred)\n",
    "            y_temp2 = -alpha_t * y_temp \n",
    "            normalized_w_t = np.multiply(w_t, np.exp(y_temp2))\n",
    "\n",
    "            #normalizing the weigths for the sum to be equal do 1\n",
    "            w_t = normalized_w_t / sum(normalized_w_t)\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            self.estimator_errors.append(eps)\n",
    "\n",
    "            \n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict output of Adaboost \n",
    "        Paramters: \n",
    "            X: array: data\n",
    "        Return: \n",
    "            y_pred: array: data\n",
    "        \"\"\"\n",
    "        #The final prediction is a compromise between all the weak learners predictions\n",
    "        list_y_pred = []\n",
    "        \n",
    "        #for each weak learner get their prediction\n",
    "\n",
    "        for WL, w in zip(self.list_WL, self.list_alpha):\n",
    "            #Final prediction is obtained by the weighted by alpha sum of each weak learner prediction\n",
    "            list_y_pred.append(WL.predict(X) * w)\n",
    "         \n",
    "        #the array of all the predictions\n",
    "\n",
    "        arr_y_pred = np.array(sum(list_y_pred))\n",
    " \n",
    "        #get -1 if y_pred < 0 or 1 if y_pred > 0\n",
    "        y_pred = np.sign(arr_y_pred)\n",
    "        \n",
    "        return y_pred \n",
    "        \n",
    "    def error_wl(self, w_t, y_pred, y):\n",
    "        \"\"\"\n",
    "        error of current weaklearner\n",
    "        Parameters:\n",
    "            w_t: array:  weight of observation\n",
    "            y_pred: array: output of wl \n",
    "            y: array: labels\n",
    "        Return: \n",
    "            eps: float: error of wl \n",
    "        \"\"\"\n",
    "        \n",
    "        ind_err = []\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i] != y[i]:\n",
    "                ind_err.append(1) \n",
    "            else: \n",
    "                ind_err.append(0) \n",
    "    \n",
    "        w_ind_err = np.multiply(w_t,ind_err)\n",
    "        \n",
    "        eps = np.sum(w_ind_err)\n",
    "    \n",
    "        return eps\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e3866a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BinaryClassAdaboost at 0x1557d592cd0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BinaryClassAdaboost(70)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ed04ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e41872ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4453333333333333,\n",
       " 0.45503224320589164,\n",
       " 0.44445759445665955,\n",
       " 0.4556735130262076,\n",
       " 0.44941358271011267,\n",
       " 0.4580110239318191,\n",
       " 0.4557613595596748,\n",
       " 0.4618767897781757,\n",
       " 0.4497835001423618,\n",
       " 0.4591059982372616,\n",
       " 0.442193423963022,\n",
       " 0.4569609939511425,\n",
       " 0.4516404681312472,\n",
       " 0.45984163358780866,\n",
       " 0.459888454839247,\n",
       " 0.4644383851560971,\n",
       " 0.45049811411202706,\n",
       " 0.4595648556904214,\n",
       " 0.45264116872913435,\n",
       " 0.4604456926222529,\n",
       " 0.45471180437785697,\n",
       " 0.4615909526179792,\n",
       " 0.4500300577869357,\n",
       " 0.45936422023190715,\n",
       " 0.4600126868903682,\n",
       " 0.46461123573077373,\n",
       " 0.4555243343892089,\n",
       " 0.46195397025026275,\n",
       " 0.4585606907194274,\n",
       " 0.4636469591937208,\n",
       " 0.4654182285082159,\n",
       " 0.4683934484202562,\n",
       " 0.4592611585794638,\n",
       " 0.4644006169148185,\n",
       " 0.46910109786572474,\n",
       " 0.4715570444761682,\n",
       " 0.4578928681950091,\n",
       " 0.4643728597536746,\n",
       " 0.45848266719688685,\n",
       " 0.4647095102106533,\n",
       " 0.45216495822969616,\n",
       " 0.46234833072509707,\n",
       " 0.4519100349028713,\n",
       " 0.4625290597868963,\n",
       " 0.4607495941890463,\n",
       " 0.4659156432244539,\n",
       " 0.46613355494224107,\n",
       " 0.4694813144690763,\n",
       " 0.46626106202419737,\n",
       " 0.4695884410208676,\n",
       " 0.4569451595487073,\n",
       " 0.46420950413104045,\n",
       " 0.4698904029721298,\n",
       " 0.4678769240555025,\n",
       " 0.4493043557732538,\n",
       " 0.45169772021330545,\n",
       " 0.46300442796830044,\n",
       " 0.46912340697229166,\n",
       " 0.4694260392520846,\n",
       " 0.4494049481708731,\n",
       " 0.4708823073769761,\n",
       " 0.47308520912290364,\n",
       " 0.47730811806612916,\n",
       " 0.47835046553667293,\n",
       " 0.46541995573321887,\n",
       " 0.4692458775818863,\n",
       " 0.464465437428578,\n",
       " 0.46865401588662997,\n",
       " 0.46649379516802847,\n",
       " 0.47002942302562356]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimator_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8345a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "36ac4fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8084"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3af64f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Multiclass LASSIFICATIONS ---------------------- #\n",
    "\n",
    "\n",
    "class MultiClassAdaBoost(object):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    base_estimator: object\n",
    "        The base model from which the boosted ensemble is built.\n",
    "    n_estimators: integer, optional(default=50)\n",
    "        The maximum number of estimators\n",
    "    learning_rate: float, optional(default=1)\n",
    "    Attributes\n",
    "    -------------\n",
    "    estimators_: list of base estimators\n",
    "    estimator_weights_: array of floats\n",
    "        Weights for each base_estimator\n",
    "    estimator_errors_: array of floats\n",
    "        Classification error for each estimator in the boosted ensemble.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.estimator_errors = []\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "        \n",
    "        # So in boost we have to ensure that the predict results have the same classes sort\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "          \n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X, y, sample_weight=w_t)\n",
    "            \n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            incorrect = y_pred != y\n",
    "            estimator_error = np.dot(incorrect, w_t) / np.sum(w_t, axis=0)\n",
    "            \n",
    "            # if worse than random guess, stop boosting\n",
    "            if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "                break\n",
    "\n",
    "            # update alphe performance\n",
    "            alpha_t = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "        \n",
    "\n",
    "            # update sample weight\n",
    "            w_t *= np.exp(alpha_t * incorrect)\n",
    "            sample_weight_sum = np.sum(w_t, axis=0)\n",
    "\n",
    "            # normalize sample weight\n",
    "            w_t /= sample_weight_sum\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            # append error\n",
    "            self.estimator_errors.append(estimator_error)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "\n",
    "        \n",
    "        pred = sum((estimator.predict(X) == classes).T * w\n",
    "                   for estimator, w in zip(self.list_WL,\n",
    "                                           self.list_alpha))\n",
    "\n",
    "        pred /= sum(self.list_alpha)\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "76444f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_quantiles(\n",
    "    n_samples=13000, n_features=10, n_classes=3, random_state=1\n",
    ")\n",
    "\n",
    "n_split = 3000\n",
    "\n",
    "X_train, X_test = X[:n_split], X[n_split:]\n",
    "y_train, y_test = y[:n_split], y[n_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dedf91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassAdaBoost(100, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "17ae1608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultiClassAdaBoost at 0x1557d6154f0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "579a0c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6206666666666666,\n",
       " 0.5607244464445126,\n",
       " 0.5425086325243259,\n",
       " 0.6135858385334544,\n",
       " 0.5420652603391231,\n",
       " 0.5753284241536961,\n",
       " 0.5980185330265171,\n",
       " 0.5228302785275332,\n",
       " 0.6149390808877095,\n",
       " 0.5830957803836047,\n",
       " 0.528565635420615,\n",
       " 0.621167852390974,\n",
       " 0.558744002173177,\n",
       " 0.5518668979561658,\n",
       " 0.5880170445179811,\n",
       " 0.5753958658886382,\n",
       " 0.5648511884839151,\n",
       " 0.574048731816833,\n",
       " 0.5547125312859807,\n",
       " 0.610022567747886,\n",
       " 0.5683839167090066,\n",
       " 0.5269065279833075,\n",
       " 0.5941977070913176,\n",
       " 0.5890595919048741,\n",
       " 0.5308747449010472,\n",
       " 0.5865884718972433,\n",
       " 0.5716288377529896,\n",
       " 0.566215104511706,\n",
       " 0.5827911724637516,\n",
       " 0.5568144831692023,\n",
       " 0.5893321469449455,\n",
       " 0.5688988919088552,\n",
       " 0.5691204835172557,\n",
       " 0.5934365368086099,\n",
       " 0.5577232500786842,\n",
       " 0.5573800302804418,\n",
       " 0.5853415762196916,\n",
       " 0.6032579124770993,\n",
       " 0.5350394212701343,\n",
       " 0.5668446094103218,\n",
       " 0.5886818493781664,\n",
       " 0.580121140895584,\n",
       " 0.5514267565014686,\n",
       " 0.574535257001408,\n",
       " 0.5859925923893254,\n",
       " 0.5623737841690859,\n",
       " 0.5631126216695282,\n",
       " 0.5725577156821924,\n",
       " 0.6061434500759028,\n",
       " 0.5380005732374797,\n",
       " 0.5624135586359534,\n",
       " 0.5876231738589817,\n",
       " 0.583422346911398,\n",
       " 0.5492727735504498,\n",
       " 0.567063363035305,\n",
       " 0.5910812923117489,\n",
       " 0.5745322328682835,\n",
       " 0.5653905386559089,\n",
       " 0.5739973446501385,\n",
       " 0.5990950911663149,\n",
       " 0.5734059973201905,\n",
       " 0.5468301290934358,\n",
       " 0.6256204042952879,\n",
       " 0.5617607139443073,\n",
       " 0.527860364775166,\n",
       " 0.6048174902976723,\n",
       " 0.5834974187239229,\n",
       " 0.541319328517207,\n",
       " 0.5898316328849332,\n",
       " 0.5703814849275278,\n",
       " 0.5720160481485235,\n",
       " 0.5828680898046794,\n",
       " 0.5677411247991994,\n",
       " 0.5884293952776934,\n",
       " 0.5753262098606867,\n",
       " 0.5435004209462977,\n",
       " 0.6126959963647254,\n",
       " 0.5784661023967168,\n",
       " 0.5523195936268694,\n",
       " 0.6124157100326163,\n",
       " 0.5539043523184138,\n",
       " 0.5705078581070577,\n",
       " 0.5983142968262289,\n",
       " 0.5427122054450659,\n",
       " 0.5957648439600541,\n",
       " 0.5746167929850362,\n",
       " 0.5786474668945575,\n",
       " 0.5928410716176525,\n",
       " 0.5582189444926657,\n",
       " 0.5681916648994649,\n",
       " 0.5865704003933108,\n",
       " 0.5800676465435488,\n",
       " 0.5582965794335919,\n",
       " 0.5804492234073674,\n",
       " 0.5872454670832918,\n",
       " 0.5588294254794185,\n",
       " 0.5709153514771405,\n",
       " 0.5777789383903671,\n",
       " 0.592999093696335,\n",
       " 0.5563228715496359]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimator_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "96fcb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16147ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6443"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore Weak Learner (use different weak learners or with different parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae94c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore Number of Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff054159",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comparing with other algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn adaboost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
