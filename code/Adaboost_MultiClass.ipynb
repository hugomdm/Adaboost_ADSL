{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html\n",
    "#https://github.com/jinxin0924/multi-adaboost/blob/master/multi_AdaBoost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93940bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765b67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_quantiles(\n",
    "    n_samples=13000, n_features=10, n_classes=3, random_state=1\n",
    ")\n",
    "y = np.where(y==0,-1,1)\n",
    "\n",
    "n_split = 3000\n",
    "\n",
    "X_train, X_test = X[:n_split], X[n_split:]\n",
    "y_train, y_test = y[:n_split], y[n_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa747cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------- BINARY CLASSIFICATIONS ---------------------- #\n",
    "\n",
    "class BinaryClassAdaboost():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators:int):\n",
    "        \"\"\"\n",
    "        Initialialisation of Adaboost class\n",
    "        Parameters: \n",
    "            n_estimators: int:  number of weak learners \n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model \n",
    "        Parameters: \n",
    "            X: array: data\n",
    "            y: array: vector of class labels where yi E Y= {1,..., k} and k = 2\n",
    "        \"\"\"\n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "              \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "\n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1,)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X, y, sample_weight=w_t)\n",
    "            #Get the predicted classes\n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            eps = self.error_wl(w_t, y_pred, y)\n",
    "        \n",
    "            # if the error of the weak learner is higher then 0.5 (worse then random guess) \n",
    "            #don't take into account this learner weight\n",
    "            if eps > 0.5:\n",
    "                break\n",
    "            \n",
    "            #Step 4: Calculate the performance of the weak learner\n",
    "            #Performance of the weak learner(α) = 0.5* ln (1 – error/error)\n",
    "            #Calculate alpha for this weak learner\n",
    "            alpha_t = 0.5 * log((1- eps) / eps)\n",
    "            \n",
    "            \n",
    "            #Step 5: Update weight\n",
    "            #With the alpha performance (α) the weights of the wrongly classified records are increased\n",
    "            #and the weights of the correctly classified records decreased.\n",
    "            y_temp = np.multiply(y, y_pred)\n",
    "            y_temp2 = -alpha_t * y_temp \n",
    "            w_t = np.multiply(w_t, np.exp(y_temp2))\n",
    "\n",
    "            #normalizing the weigths for the sum to be equal do 1\n",
    "            w_t = w_t / sum(w_t)\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            \n",
    "            \n",
    "            \n",
    "        return 1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict output of Adaboost \n",
    "        Paramters: \n",
    "            X: array: data\n",
    "        Return: \n",
    "            y_pred: array: data\n",
    "        \"\"\"\n",
    "        #The final prediction is a compromise between all the weak learners predictions\n",
    "        list_y_pred = []\n",
    "        \n",
    "        #for each weak learner get their prediction\n",
    "        for WL in self.list_WL:\n",
    "            list_y_pred.append(WL.predict(X))\n",
    "         \n",
    "        #the array of all the predictions\n",
    "        arr_y_pred = np.array(list_y_pred)\n",
    "        \n",
    "        #Final prediction is obtained by the weighted by alpha sum of each weak learner prediction\n",
    "        weighted_sum_pred = np.sum(np.multiply(arr_y_pred, self.list_alpha), axis = 0)\n",
    "        y_pred_resized = np.reshape(weighted_sum_pred, (weighted_sum_pred.shape[0],1))\n",
    "        \n",
    "        #get -1 if y_pred < 0 or 1 if y_pred > 0\n",
    "        y_pred = np.sign(y_pred_resized)\n",
    "        \n",
    "        return y_pred \n",
    "        \n",
    "    def error_wl(self, w_t, y_pred, y):\n",
    "        \"\"\"\n",
    "        error of current weaklearner\n",
    "        Parameters:\n",
    "            w_t: array:  weight of observation\n",
    "            y_pred: array: output of wl \n",
    "            y: array: labels\n",
    "        Return: \n",
    "            eps: float: error of wl \n",
    "        \"\"\"\n",
    "        \n",
    "        ind_err = []\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i] != y[i]:\n",
    "                ind_err.append(1) \n",
    "            else: \n",
    "                ind_err.append(0) \n",
    "    \n",
    "        w_ind_err = np.multiply(w_t,ind_err)\n",
    "        \n",
    "        eps = np.sum(w_ind_err)\n",
    "    \n",
    "        return eps\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346102a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------- BINARY CLASSIFICATIONS WITH SAMPLING ---------------------- #\n",
    "\n",
    "class BinaryClassAdaboost():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators:int):\n",
    "        \"\"\"\n",
    "        Initialialisation of Adaboost class\n",
    "        Parameters: \n",
    "            n_estimators: int:  number of weak learners \n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model \n",
    "        Parameters: \n",
    "            X: array: data\n",
    "            y: array: vector of class labels where yi E Y= {1,..., k} and k = 2\n",
    "        \"\"\"\n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "              \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "            \n",
    "            #Draw random samples with replacement from original data   \n",
    "            #with the updated weigths\n",
    "            X_sample, y_sample = self.sampling(X, y, w_t)\n",
    "            \n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1,)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X_sample, y_sample, sample_weight=w_t)\n",
    "            #Get the predicted classes\n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            eps = self.error_wl(w_t, y_pred, y)\n",
    "        \n",
    "            # if the error of the weak learner is higher then 0.5 (worse then random guess) \n",
    "            #don't take into account this learner weight\n",
    "            if eps > 0.5:\n",
    "                break\n",
    "            \n",
    "            #Step 4: Calculate the performance of the weak learner\n",
    "            #Performance of the weak learner(α) = 0.5* ln (1 – error/error)\n",
    "            #Calculate alpha for this weak learner\n",
    "            alpha_t = 0.5 * log((1- eps) / eps)\n",
    "            \n",
    "            \n",
    "            #Step 5: Update weight\n",
    "            #With the alpha performance (α) the weights of the wrongly classified records are increased\n",
    "            #and the weights of the correctly classified records decreased.\n",
    "            y_temp = np.multiply(y, y_pred)\n",
    "            y_temp2 = -alpha_t * y_temp \n",
    "            w_t = np.multiply(w_t, np.exp(y_temp2))\n",
    "\n",
    "            #normalizing the weigths for the sum to be equal do 1\n",
    "            w_t = w_t / sum(w_t)\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            \n",
    "            \n",
    "            \n",
    "        return 1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict output of Adaboost \n",
    "        Paramters: \n",
    "            X: array: data\n",
    "        Return: \n",
    "            y_pred: array: data\n",
    "        \"\"\"\n",
    "        #The final prediction is a compromise between all the weak learners predictions\n",
    "        list_y_pred = []\n",
    "        \n",
    "        #for each weak learner get their prediction\n",
    "        for WL in self.list_WL:\n",
    "            list_y_pred.append(WL.predict(X))\n",
    "         \n",
    "        #the array of all the predictions\n",
    "        arr_y_pred = np.array(list_y_pred)\n",
    "        \n",
    "        #Final prediction is obtained by the weighted by alpha sum of each weak learner prediction\n",
    "        weighted_sum_pred = np.sum(np.multiply(arr_y_pred, self.list_alpha), axis = 0)\n",
    "        y_pred_resized = np.reshape(weighted_sum_pred, (weighted_sum_pred.shape[0],1))\n",
    "        \n",
    "        #get -1 if y_pred < 0 or 1 if y_pred > 0\n",
    "        y_pred = np.sign(y_pred_resized)\n",
    "        \n",
    "        return y_pred \n",
    "        \n",
    "    def error_wl(self, w_t, y_pred, y):\n",
    "        \"\"\"\n",
    "        error of current weaklearner\n",
    "        Parameters:\n",
    "            w_t: array:  weight of observation\n",
    "            y_pred: array: output of wl \n",
    "            y: array: labels\n",
    "        Return: \n",
    "            eps: float: error of wl \n",
    "        \"\"\"\n",
    "        \n",
    "        ind_err = []\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i] != y[i]:\n",
    "                ind_err.append(1) \n",
    "            else: \n",
    "                ind_err.append(0) \n",
    "    \n",
    "        w_ind_err = np.multiply(w_t,ind_err)\n",
    "        \n",
    "        eps = np.sum(w_ind_err)\n",
    "    \n",
    "        return eps\n",
    "    \n",
    "        \n",
    "    def sampling(self, X, y, w_t):\n",
    "        \"\"\"\n",
    "        sampling X with w_t \n",
    "        Parameters:\n",
    "            X: array: data\n",
    "            y: array: labels\n",
    "            w_t: array: weigth\n",
    "        Return:\n",
    "            X_sample: array: sample of X\n",
    "            y_sample: array: labels corresponding to X_sample\n",
    "        \"\"\"\n",
    "        \n",
    "        '''As for our implementaion of AdaBoost \n",
    "        y needs to be in {-1,1}\n",
    "        '''\n",
    "        y = np.where(y==0,-1,1)\n",
    "        \n",
    "        #put X and y in same array to sample \n",
    "        y_temp = np.reshape(y, (y.shape[0], 1))\n",
    "        \n",
    "        data = np.hstack((X, y_temp))\n",
    "    \n",
    "        #size of sample\n",
    "        size = int(0.75*X.shape[0])\n",
    "        \n",
    "        #get sampled data with the weights \n",
    "        sample = random.choices(data, weights=w_t, k=size)\n",
    "        \n",
    "        #getting X and y \n",
    "        y_sample = sample[:,-1]\n",
    "        X_sample = sample[:,:-1]\n",
    "        \n",
    "        return X_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3af64f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Multiclass LASSIFICATIONS ---------------------- #\n",
    "\n",
    "\n",
    "class MultiClassAdaBoost(object):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    base_estimator: object\n",
    "        The base model from which the boosted ensemble is built.\n",
    "    n_estimators: integer, optional(default=50)\n",
    "        The maximum number of estimators\n",
    "    learning_rate: float, optional(default=1)\n",
    "    Attributes\n",
    "    -------------\n",
    "    estimators_: list of base estimators\n",
    "    estimator_weights_: array of floats\n",
    "        Weights for each base_estimator\n",
    "    estimator_errors_: array of floats\n",
    "        Classification error for each estimator in the boosted ensemble.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.estimator_errors = []\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "        \n",
    "        # So in boost we have to ensure that the predict results have the same classes sort\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "          \n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X, y, sample_weight=w_t)\n",
    "            \n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            incorrect = y_pred != y\n",
    "            estimator_error = np.dot(incorrect, w_t) / np.sum(w_t, axis=0)\n",
    "            \n",
    "            # if worse than random guess, stop boosting\n",
    "            if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "                break\n",
    "\n",
    "            # update alphe performance\n",
    "            alpha_t = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "        \n",
    "\n",
    "            # update sample weight\n",
    "            w_t *= np.exp(alpha_t * incorrect)\n",
    "            sample_weight_sum = np.sum(w_t, axis=0)\n",
    "\n",
    "            # normalize sample weight\n",
    "            w_t /= sample_weight_sum\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            # append error\n",
    "            self.estimator_errors.append(estimator_error)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "\n",
    "        \n",
    "        pred = sum((estimator.predict(X) == classes).T * w\n",
    "                   for estimator, w in zip(self.list_WL,\n",
    "                                           self.list_alpha))\n",
    "\n",
    "        pred /= sum(self.list_alpha)\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dedf91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassAdaBoost(3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17ae1608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AdaBoostClassifier at 0x2d0c7874d90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f998c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred =  np.array([0.4, 0.3, 0.5, 0.6, 0.8, 0, 1, 0.9, 0.7,0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96fcb7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_pred.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b676bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Class\n",
    "\n",
    "from sklearn import base\n",
    "\n",
    "def indexToVector(y,k,labelDict):\n",
    "    y_new = []\n",
    "    for yi in y:\n",
    "        i = labelDict[yi]\n",
    "        v = np.ones(k)*(-1/(k-1))\n",
    "        v[i] = 1\n",
    "        y_new.append(v)\n",
    "    return np.array(y_new)\n",
    "\n",
    "def indexToLabel(i,clf):\n",
    "    return clf.classes[i]\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    \n",
    "    def __init__(self,base_estimator=None,n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = [None]*n_estimators\n",
    "        if base_estimator == None:\n",
    "            base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "        self.base_estimator = base_estimator\n",
    "        self.estimator_errors_ = []\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        X = np.float64(X)\n",
    "        N = len(y)\n",
    "        w = np.array([1/N for i in range(N)])\n",
    "        \n",
    "        self.createLabelDict(np.unique(y))\n",
    "        k = len(self.classes)\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            \n",
    "            Gm = base.clone(self.base_estimator).\\\n",
    "                            fit(X,y,sample_weight=w).predict\n",
    "            \n",
    "            incorrect = Gm(X) != y\n",
    "            errM = np.average(incorrect,weights=w,axis=0)\n",
    "            \n",
    "            self.estimator_errors_.append(errM)\n",
    "            \n",
    "            BetaM = (np.log((1-errM)/errM)+np.log(k-1))\n",
    "            \n",
    "            w *= np.exp(BetaM*incorrect*(w > 0))\n",
    "            \n",
    "            self.models[m] = (BetaM,Gm)\n",
    "            \n",
    "    def createLabelDict(self,classes):\n",
    "        self.labelDict = {}\n",
    "        self.classes = classes\n",
    "        for i,cl in enumerate(classes):\n",
    "            self.labelDict[cl] = i\n",
    "\n",
    "    def predict(self,X):\n",
    "        k = len(self.classes)\n",
    "        y_pred = sum(Bm*indexToVector(Gm(X),k,self.labelDict) \\\n",
    "                             for Bm,Gm in self.models)\n",
    "        \n",
    "        iTL = np.vectorize(indexToLabel)\n",
    "        return iTL(np.argmax(y_pred,axis=1),self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore Weak Learner (use different weak learners or with different parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae94c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore Number of Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff054159",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comparing with other algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn adaboost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
