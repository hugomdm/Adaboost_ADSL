{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f728388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html\n",
    "#https://github.com/jinxin0924/multi-adaboost/blob/master/multi_AdaBoost.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "93940bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "765b67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_quantiles(\n",
    "    n_samples=13000, n_features=10, n_classes=2, random_state=1\n",
    ")\n",
    "\n",
    "y = np.where(y==0,-1,1)\n",
    "\n",
    "n_split = 3000\n",
    "\n",
    "X_train, X_test = X[:n_split], X[n_split:]\n",
    "y_train, y_test = y[:n_split], y[n_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aa5e0778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a binary \n",
    "y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4aa747cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------- BINARY CLASSIFICATIONS ---------------------- #\n",
    "\n",
    "class BinaryClassAdaboost():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators:int):\n",
    "        \"\"\"\n",
    "        Initialialisation of Adaboost class\n",
    "        Parameters: \n",
    "            n_estimators: int:  number of weak learners \n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        self.estimator_errors = []\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model \n",
    "        Parameters: \n",
    "            X: array: data\n",
    "            y: array: vector of class labels where yi E Y= {1,..., k} and k = 2\n",
    "        \"\"\"\n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "              \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "\n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X, y, sample_weight=w_t)\n",
    "            #Get the predicted classes\n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            eps = self.error_wl(w_t, y_pred, y)\n",
    "        \n",
    "            # if the error of the weak learner is higher then 0.5 (worse then random guess) \n",
    "            #don't take into account this learner weight\n",
    "            if eps > 0.5:\n",
    "                break\n",
    "            \n",
    "            #Step 4: Calculate the performance of the weak learner\n",
    "            #Performance of the weak learner(α) = 0.5* ln (1 – error/error)\n",
    "            #Calculate alpha for this weak learner\n",
    "            \n",
    "            alpha_t = 0.5 * np.log((1- eps) / eps)\n",
    "            \n",
    "\n",
    "            #Step 5: Update weight\n",
    "            #With the alpha performance (α) the weights of the wrongly classified records are increased\n",
    "            #and the weights of the correctly classified records decreased.\n",
    "            y_temp = np.multiply(y, y_pred)\n",
    "            y_temp2 = -alpha_t * y_temp \n",
    "            normalized_w_t = np.multiply(w_t, np.exp(y_temp2))\n",
    "\n",
    "            #normalizing the weigths for the sum to be equal do 1\n",
    "            w_t = normalized_w_t / sum(normalized_w_t)\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            self.estimator_errors.append(eps)\n",
    "\n",
    "            \n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict output of Adaboost \n",
    "        Paramters: \n",
    "            X: array: data\n",
    "        Return: \n",
    "            y_pred: array: data\n",
    "        \"\"\"\n",
    "        #The final prediction is a compromise between all the weak learners predictions\n",
    "        list_y_pred = []\n",
    "        \n",
    "        #for each weak learner get their prediction\n",
    "\n",
    "        for WL, w in zip(self.list_WL, self.list_alpha):\n",
    "            #Final prediction is obtained by the weighted by alpha sum of each weak learner prediction\n",
    "            list_y_pred.append(WL.predict(X) * w)\n",
    "         \n",
    "        #the array of all the predictions\n",
    "\n",
    "        arr_y_pred = np.array(sum(list_y_pred))\n",
    " \n",
    "        #get -1 if y_pred < 0 or 1 if y_pred > 0\n",
    "        y_pred = np.sign(arr_y_pred)\n",
    "        \n",
    "        return y_pred \n",
    "        \n",
    "    def error_wl(self, w_t, y_pred, y):\n",
    "        \"\"\"\n",
    "        error of current weaklearner\n",
    "        Parameters:\n",
    "            w_t: array:  weight of observation\n",
    "            y_pred: array: output of wl \n",
    "            y: array: labels\n",
    "        Return: \n",
    "            eps: float: error of wl \n",
    "        \"\"\"\n",
    "        \n",
    "        ind_err = []\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i] != y[i]:\n",
    "                ind_err.append(1) \n",
    "            else: \n",
    "                ind_err.append(0) \n",
    "    \n",
    "        w_ind_err = np.multiply(w_t,ind_err)\n",
    "        \n",
    "        eps = np.sum(w_ind_err)\n",
    "    \n",
    "        return eps\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c249c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BinaryClassAdaboost at 0x1557d5ee760>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BinaryClassAdaboost(50)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b6dc5764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "31c843a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4453333333333333,\n",
       " 0.45503224320589164,\n",
       " 0.44445759445665955,\n",
       " 0.4556735130262076,\n",
       " 0.44941358271011267,\n",
       " 0.4580110239318191,\n",
       " 0.4557613595596748,\n",
       " 0.4618767897781757,\n",
       " 0.4497835001423618,\n",
       " 0.4591059982372616,\n",
       " 0.442193423963022,\n",
       " 0.4569609939511425,\n",
       " 0.4516404681312472,\n",
       " 0.45984163358780866,\n",
       " 0.459888454839247,\n",
       " 0.4644383851560971,\n",
       " 0.45049811411202706,\n",
       " 0.4595648556904214,\n",
       " 0.45264116872913435,\n",
       " 0.4604456926222529,\n",
       " 0.45471180437785697,\n",
       " 0.4615909526179792,\n",
       " 0.4500300577869357,\n",
       " 0.45936422023190715,\n",
       " 0.4600126868903682,\n",
       " 0.46461123573077373,\n",
       " 0.4555243343892089,\n",
       " 0.46195397025026275,\n",
       " 0.4585606907194274,\n",
       " 0.4636469591937208,\n",
       " 0.4654182285082159,\n",
       " 0.4683934484202562,\n",
       " 0.4592611585794638,\n",
       " 0.4644006169148185,\n",
       " 0.46910109786572474,\n",
       " 0.4715570444761682,\n",
       " 0.4578928681950091,\n",
       " 0.4643728597536746,\n",
       " 0.45848266719688685,\n",
       " 0.4647095102106533,\n",
       " 0.45216495822969616,\n",
       " 0.46234833072509707,\n",
       " 0.4519100349028713,\n",
       " 0.4625290597868963,\n",
       " 0.4607495941890463,\n",
       " 0.4659156432244539,\n",
       " 0.46613355494224107,\n",
       " 0.4694813144690763,\n",
       " 0.46626106202419737,\n",
       " 0.4695884410208676]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimator_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "be2ea530",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5db82e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7797"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3af64f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------Multiclass LASSIFICATIONS ---------------------- #\n",
    "\n",
    "\n",
    "class MultiClassAdaBoost(object):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    base_estimator: object\n",
    "        The base model from which the boosted ensemble is built.\n",
    "    n_estimators: integer, optional(default=50)\n",
    "        The maximum number of estimators\n",
    "    learning_rate: float, optional(default=1)\n",
    "    Attributes\n",
    "    -------------\n",
    "    estimators_: list of base estimators\n",
    "    estimator_weights_: array of floats\n",
    "        Weights for each base_estimator\n",
    "    estimator_errors_: array of floats\n",
    "        Classification error for each estimator in the boosted ensemble.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.list_WL = [] #list with model\n",
    "        self.list_alpha = [] #list with weight of model \n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.estimator_errors = []\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        ## Step 1: Initialize the weights to a constant\n",
    "        n_samples = X.shape[0]                \n",
    "        w = []\n",
    "        ##Weights are initialized to 1/Number of samples: \n",
    "        w_t = [1/n_samples for x in range(n_samples)]       \n",
    "        \n",
    "        # So in boost we have to ensure that the predict results have the same classes sort\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        \n",
    "        \n",
    "        ## Step 2: Classify with ramdom sampling of data using a weak learner\n",
    "        #Construction des weaklearner\n",
    "        \n",
    "        #for each weak learner\n",
    "        for t in range(self.n_estimators):\n",
    "          \n",
    "            #Choose and Call the Base/Weak learner\n",
    "            #A decision tree with one depth has one node and is called a stump or weak learner\n",
    "            WL = DecisionTreeClassifier(max_depth=1)\n",
    "            #Fit the stump model with the ramdom samples\n",
    "            WL.fit(X, y, sample_weight=w_t)\n",
    "            \n",
    "            y_pred = WL.predict(X)\n",
    "            \n",
    "            ##Step 3: Compute error of weak learner\n",
    "            incorrect = y_pred != y\n",
    "            estimator_error = np.dot(incorrect, w_t) / np.sum(w_t, axis=0)\n",
    "            \n",
    "            # if worse than random guess, stop boosting\n",
    "            if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "                break\n",
    "\n",
    "            # update alphe performance\n",
    "            alpha_t = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "        \n",
    "\n",
    "            # update sample weight\n",
    "            w_t *= np.exp(alpha_t * incorrect)\n",
    "            sample_weight_sum = np.sum(w_t, axis=0)\n",
    "\n",
    "            # normalize sample weight\n",
    "            w_t /= sample_weight_sum\n",
    "            \n",
    "            #store the alpha performance of each weak learner\n",
    "            self.list_alpha.append(alpha_t)\n",
    "            #store each weak learner\n",
    "            self.list_WL.append(WL)\n",
    "            # append error\n",
    "            self.estimator_errors.append(estimator_error)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "\n",
    "        \n",
    "        pred = sum((estimator.predict(X) == classes).T * w\n",
    "                   for estimator, w in zip(self.list_WL,\n",
    "                                           self.list_alpha))\n",
    "\n",
    "        pred /= sum(self.list_alpha)\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2804224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_gaussian_quantiles(\n",
    "    n_samples=13000, n_features=10, n_classes=3, random_state=1\n",
    ")\n",
    "\n",
    "n_split = 3000\n",
    "\n",
    "X_train, X_test = X[:n_split], X[n_split:]\n",
    "y_train, y_test = y[:n_split], y[n_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dedf91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassAdaBoost(50, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "17ae1608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultiClassAdaBoost at 0x1557d644e50>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dc57dbed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6206666666666666,\n",
       " 0.5607244464445126,\n",
       " 0.5425086325243259,\n",
       " 0.6135858385334544,\n",
       " 0.5420652603391231,\n",
       " 0.5753284241536961,\n",
       " 0.5980185330265171,\n",
       " 0.5228302785275332,\n",
       " 0.6149390808877095,\n",
       " 0.5830957803836047,\n",
       " 0.528565635420615,\n",
       " 0.621167852390974,\n",
       " 0.558744002173177,\n",
       " 0.5518668979561658,\n",
       " 0.5880170445179811,\n",
       " 0.5753958658886382,\n",
       " 0.5648511884839151,\n",
       " 0.574048731816833,\n",
       " 0.5547125312859807,\n",
       " 0.610022567747886,\n",
       " 0.5683839167090066,\n",
       " 0.5269065279833075,\n",
       " 0.5941977070913176,\n",
       " 0.5890595919048741,\n",
       " 0.5308747449010472,\n",
       " 0.5865884718972433,\n",
       " 0.5716288377529896,\n",
       " 0.566215104511706,\n",
       " 0.5827911724637516,\n",
       " 0.5568144831692023,\n",
       " 0.5893321469449455,\n",
       " 0.5688988919088552,\n",
       " 0.5691204835172557,\n",
       " 0.5934365368086099,\n",
       " 0.5577232500786842,\n",
       " 0.5573800302804418,\n",
       " 0.5853415762196916,\n",
       " 0.6032579124770993,\n",
       " 0.5350394212701343,\n",
       " 0.5668446094103218,\n",
       " 0.5886818493781664,\n",
       " 0.580121140895584,\n",
       " 0.5514267565014686,\n",
       " 0.574535257001408,\n",
       " 0.5859925923893254,\n",
       " 0.5623737841690859,\n",
       " 0.5631126216695282,\n",
       " 0.5725577156821924,\n",
       " 0.6061434500759028,\n",
       " 0.5380005732374797]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimator_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "96fcb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "699cffcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4919"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore Weak Learner (use different weak learners or with different parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae94c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore Number of Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "605a5ce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [159]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Optimizing \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m start_time\u001b[38;5;241m=\u001b[39m\u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m make_pipeline(RandomForestClassifier())\n\u001b[0;32m      5\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m   {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomforestclassifier__bootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m], \n\u001b[0;32m      7\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomforestclassifier__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m110\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m  ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "## Optimizing \n",
    "start_time=time.time()\n",
    "\n",
    "pipe = make_pipeline(RandomForestClassifier())\n",
    "param_grid = [\n",
    "  {'randomforestclassifier__bootstrap': [True, False], \n",
    "   'randomforestclassifier__max_depth': [80, 90, 100, 110],\n",
    "   'randomforestclassifier__max_features': ['auto', 'log2'],\n",
    "   'randomforestclassifier__n_estimators': [100, 200, 300, 1000]\n",
    "  }\n",
    "    \n",
    " ]\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='accuracy')\n",
    "\n",
    "grid3 = grid.fit(X, labels)\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ff054159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Support Vector Classifier</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <th>Adaboost Classifier</th>\n",
       "      <th>Best Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.641923</td>\n",
       "      <td>0.793385</td>\n",
       "      <td>0.913077</td>\n",
       "      <td>0.734615</td>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.335082</td>\n",
       "      <td>0.334982</td>\n",
       "      <td>0.645245</td>\n",
       "      <td>0.796442</td>\n",
       "      <td>0.928256</td>\n",
       "      <td>0.789048</td>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.641923</td>\n",
       "      <td>0.793385</td>\n",
       "      <td>0.913077</td>\n",
       "      <td>0.734615</td>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.330451</td>\n",
       "      <td>0.330050</td>\n",
       "      <td>0.643063</td>\n",
       "      <td>0.794242</td>\n",
       "      <td>0.914871</td>\n",
       "      <td>0.740526</td>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Logistic Regression  Support Vector Classifier  Decision Tree  \\\n",
       "Accuracy              0.335000                   0.335000       0.641923   \n",
       "Precision             0.335082                   0.334982       0.645245   \n",
       "Recall                0.335000                   0.335000       0.641923   \n",
       "F1 Score              0.330451                   0.330050       0.643063   \n",
       "\n",
       "           Random Forest  Gaussian Naive Bayes  Adaboost Classifier  \\\n",
       "Accuracy        0.793385              0.913077             0.734615   \n",
       "Precision       0.796442              0.928256             0.789048   \n",
       "Recall          0.793385              0.913077             0.734615   \n",
       "F1 Score        0.794242              0.914871             0.740526   \n",
       "\n",
       "                     Best Score  \n",
       "Accuracy   Gaussian Naive Bayes  \n",
       "Precision  Gaussian Naive Bayes  \n",
       "Recall     Gaussian Naive Bayes  \n",
       "F1 Score   Gaussian Naive Bayes  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Comparing with other algo\n",
    "\n",
    "# Define dictionary with performance metrics\n",
    "\n",
    "scores = ['accuracy','precision_weighted','recall_weighted','f1_weighted',]\n",
    "# Instantiate the machine learning classifiers\n",
    "log_model = LogisticRegression(max_iter=10000)\n",
    "svc_model = LinearSVC(dual=False)\n",
    "dtr_model = DecisionTreeClassifier()\n",
    "rfc_model = RandomForestClassifier()\n",
    "gnb_model = GaussianNB()\n",
    "ada_model = AdaBoostClassifier()\n",
    "#my_ada_model = MultiClassAdaBoost(100, 0.001)\n",
    "\n",
    "\n",
    "# Define the models evaluation function\n",
    "def models_evaluation(X, labels, folds):\n",
    "    \n",
    "    '''\n",
    "    X : data set features\n",
    "    labels : data set target\n",
    "    folds : number of cross-validation folds\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Perform cross-validation to each machine learning classifier\n",
    "    log = cross_validate(log_model, X, labels, cv=folds, scoring=scores)\n",
    "    svc = cross_validate(svc_model, X, labels, cv=folds, scoring=scores)\n",
    "    dtr = cross_validate(dtr_model, X, labels, cv=folds, scoring=scores)\n",
    "    rfc = cross_validate(rfc_model, X, labels, cv=folds, scoring=scores)\n",
    "    gnb = cross_validate(gnb_model, X, labels, cv=folds, scoring=scores)\n",
    "    ada = cross_validate(ada_model, X, labels, cv=folds, scoring=scores)\n",
    "    #my_ada = cross_validate(my_ada_model, X, labels, cv=folds, scoring=scores)\n",
    "\n",
    "    \n",
    "    # Create a data frame with the models perfoamnce metrics scores\n",
    "    models_scores_table = pd.DataFrame({'Logistic Regression':[log['test_accuracy'].mean(),\n",
    "                                                               log['test_precision_weighted'].mean(),\n",
    "                                                               log['test_recall_weighted'].mean(),\n",
    "                                                               log['test_f1_weighted'].mean()],\n",
    "                                       \n",
    "                                      'Support Vector Classifier':[svc['test_accuracy'].mean(),\n",
    "                                                                   svc['test_precision_weighted'].mean(),\n",
    "                                                                   svc['test_recall_weighted'].mean(),\n",
    "                                                                   svc['test_f1_weighted'].mean()],\n",
    "                                       \n",
    "                                      'Decision Tree':[dtr['test_accuracy'].mean(),\n",
    "                                                       dtr['test_precision_weighted'].mean(),\n",
    "                                                       dtr['test_recall_weighted'].mean(),\n",
    "                                                       dtr['test_f1_weighted'].mean()],\n",
    "                                       \n",
    "                                      'Random Forest':[rfc['test_accuracy'].mean(),\n",
    "                                                       rfc['test_precision_weighted'].mean(),\n",
    "                                                       rfc['test_recall_weighted'].mean(),\n",
    "                                                       rfc['test_f1_weighted'].mean()],\n",
    "                                       \n",
    "                                      'Gaussian Naive Bayes':[gnb['test_accuracy'].mean(),\n",
    "                                                              gnb['test_precision_weighted'].mean(),\n",
    "                                                              gnb['test_recall_weighted'].mean(),\n",
    "                                                              gnb['test_f1_weighted'].mean()], \n",
    "                                       \n",
    "                                       \n",
    "                                       'Adaboost Classifier':[ada['test_accuracy'].mean(),\n",
    "                                                              ada['test_precision_weighted'].mean(),\n",
    "                                                              ada['test_recall_weighted'].mean(),\n",
    "                                                              ada['test_f1_weighted'].mean()]\n",
    "                                       },\n",
    "                                      \n",
    "                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "    \n",
    "    # Add 'Best Score' column\n",
    "    models_scores_table['Best Score'] = models_scores_table.idxmax(axis=1)\n",
    "    \n",
    "    # Return models performance metrics scores data frame\n",
    "    return(models_scores_table)\n",
    "  \n",
    "# Run models_evaluation function\n",
    "models_evaluation(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cca76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
